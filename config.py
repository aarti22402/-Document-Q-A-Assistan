# config.py

# Ollama models
OLLAMA_LLM = "mistral"
EMBEDDING_MODEL = "nomic-embed-text"

# Text chunking configuration
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

# Retrieval configuration
TOP_K = 4
